%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Plantilla de memoria en LaTeX para la ETSIT - Universidad Rey Juan Carlos
%%
%% Por Gregorio Robles <grex arroba gsyc.urjc.es>
%%     Grupo de Sistemas y Comunicaciones
%%     Escuela Técnica Superior de Ingenieros de Telecomunicación
%%     Universidad Rey Juan Carlos
%% (muchas ideas tomadas de Internet, colegas del GSyC, antiguos alumnos...
%%  etc. Muchas gracias a todos)
%%
%% La última versión de esta plantilla está siempre disponible en:
%%     https://github.com/gregoriorobles/plantilla-memoria
%%
%% Para obtener PDF, ejecuta en la shell:
%%   make
%% (las imágenes deben ir en PNG o JPG)

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\documentclass[a4paper, 12pt]{book}
%\usepackage[T1]{fontenc}

\usepackage[a4paper, left=2.5cm, right=2.5cm, top=3cm, bottom=3cm]{geometry}
\usepackage{times}
\usepackage[latin1]{inputenc}
%\usepackage[spanish]{babel} % Comenta esta línea si tu memoria es en inglés
\usepackage[spanish,es-tabla]{babel}
\usepackage{url}
%\usepackage[dvipdfm]{graphicx}
\usepackage{graphicx}
\usepackage{float}  %% H para posicionar figuras
\usepackage[nottoc, notlot, notlof, notindex]{tocbibind} %% Opciones de índice
\usepackage{latexsym}  %% Logo LaTeX

\usepackage{hyperref}
\hypersetup{
    colorlinks,
    citecolor=black,
    filecolor=black,
    linkcolor=black,
    urlcolor=black
}

\title{Marco de trabajo para evaluar la relevancia de los artículos en el dominio científico}
\author{Adrián Alonso Barriuso}

\renewcommand{\baselinestretch}{1.5}  %% Interlineado

\begin{document}

\renewcommand{\refname}{Bibliografía}  %% Renombrando
\renewcommand{\appendixname}{Apéndice}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% PORTADA

\begin{titlepage}
\begin{center}
\begin{tabular}[c]{c c}
%\includegraphics[bb=0 0 194 352, scale=0.25]{logo} &
\includegraphics[scale=0.25]{img/logo_vect.png} &
\begin{tabular}[b]{l}
\Huge
\textsf{UNIVERSIDAD} \\
\Huge
\textsf{REY JUAN CARLOS} \\
\end{tabular}
\\
\end{tabular}

\vspace{3cm}

\Large
MÁSTER EN INGENIERÍA EN SISTEMAS DE DECISIÓN

\vspace{0.4cm}

\large
Curso Académico 2018/2019

\vspace{0.8cm}

Trabajo Fin de Máster

\vspace{2.5cm}

\LARGE
Marco de trabajo para evaluar la relevancia de los artículos en el dominio científico

\vspace{4cm}

\large
Autor : Adrián Alonso Barriuso \\
Tutor : Dr. Alberto Fernández Isabel

\end{center}
\end{titlepage}

\newpage
\mbox{}
\thispagestyle{empty} % para que no se numere esta pagina

\clearpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%% Dedicatoria

\chapter*{}
\pagenumbering{arabic}

\begin{flushright}
\textit{Dedicado a \\
mi familia, pareja, amigos y a todos los que me aguantan, en el buen sentido.}
\end{flushright}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%% Agradecimientos

\chapter*{Agradecimientos}
%\addcontentsline{toc}{chapter}{Agradecimientos} % si queremos que aparezca en el índice
\markboth{AGRADECIMIENTOS}{AGRADECIMIENTOS} % encabezado 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%% Resumen

\chapter*{Resumen}
%\addcontentsline{toc}{chapter}{Resumen} % si queremos que aparezca en el índice




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%% Resumen en inglés

\chapter*{Summary}
%\addcontentsline{toc}{chapter}{Summary} % si queremos que aparezca en el índice
\markboth{SUMMARY}{SUMMARY} % encabezado


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% ÍNDICES %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Las buenas noticias es que los índices se generan automáticamente.
% Lo único que tienes que hacer es elegir cuáles quieren que se generen,
% y comentar/descomentar esa instrucción de LaTeX.

%%%% Índice de contenidos
\tableofcontents 
%%%% Índice de figuras
\cleardoublepage
%\addcontentsline{toc}{chapter}{Lista de figuras} % para que aparezca en el indice de contenidos
\listoffigures % indice de figuras
%%%% Índice de tablas
%\cleardoublepage
%\addcontentsline{toc}{chapter}{Lista de tablas} % para que aparezca en el indice de contenidos
%\listoftables % indice de tablas


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% INTRODUCCIÓN %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\cleardoublepage
\chapter{Introducción}
\label{sec:intro} % etiqueta para poder referenciar luego en el texto con ~


\section{Contexto}
\label{sec:contexto}



\subsection{Dominio de aplicación}
\label{sec:dominio}




\section{Objetivos}
\label{sec:objetivos}



\subsection{Objetivo General}
\label{sec:obgeneral}



\subsection{Objetivos específicos}
\label{sec:obepescificos}

\begin{itemize}
\item 
\item 
\end{itemize}

\section{Estructura de la memoria}
\label{sec:estructura}

\begin{enumerate}
\item Estado del arte: 
\item Propuesta: 
\item Experimentos y resultados: 
\item Conclusiones: 
\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% ESTADO DEL ARTE %el
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\cleardoublepage
\chapter{Estado del arte}


\section{Algoritmos de reputación}
\label{sec:alrepus}


\section{Obtención de relevancias}
\label{sec:oreles}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% DISEÑO E IMPLEMENTACIÓN %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\cleardoublepage
\chapter{Propuesta}
\label{chap:propuesta}
En este capítulo, se describe la propuesta del sistema completo, definiendo entradas y salidas explicadas a nivel de diseño. Se empieza con una subsección donde se ve la arquitectura y el propósito general y después se entra en detalle para cada uno de los módulos en las subsiguientes secciones.

\section{Arquitectura general} 
\label{sec:arquitectura}
En la Figura \ref{fig:arquitectura}, se pueden observar el módulo principal: \textit{Relevance estimator module}, que es el encargado de estimar las relevancias de los artículos de entrada, y sus correspondientes submódulos: \textit{Text processor, Reputation calculator y Relevance calculator}. Se cuenta además, con dos fuentes de información precalculadas utilizadas por el submódulo \textit{Relevance calculator}, a saber, \textit{Relevance lexicon} que consiste en cuatro lexicones\cite{pustejovsky1991generative} de relevancias de términos médicos y \textit{Neural network}, que consiste en cuatro modelos entrenados de redes neuronales convolucionales (CNN)\cite{poria2015deep} por sus siglas en inglés. Por otra parte, se cuenta con un módulo de visualización(\textit{Visualization}), que se utiliza para ver la salida del sistema y para proporcionar la entrada (\textit{Text}). Por último, se utiliza una fuente de información externa en tiempo real (\textit{Web information resources}).

En primer lugar se entra en detalle en cómo se construye Relevance lexicon, después Neural network y, por último, Relevance estimator module, el cuál emplea todo lo anterior para calcular las relevancias de nuevos documentos.

\begin{figure}
  \centering
  \includegraphics[width=16cm, keepaspectratio]{img/architecture}
  \caption{Arquitectura general}
  \label{fig:arquitectura}
\end{figure}

\section{Creación del lexicón de relevancias} 
\label{sec:lex}
El lexicón de relevancias tiene un papel fundamental a la hora de estimar la relevancia de los artículos, en concreto, contiene la relevancia de las palabras de el dominio de aplicación utilizado para crear el mismo. Para la creación del lexicón se ha diseñado un marco de trabajo completo, cuya arquitectura puede verse en la Figura \ref{fig:train_lexicon}. Esta cuenta con un módulo de extracción, transformación y carga (ETL por sus siglas en inglés)\cite{vassiliadis2009survey}, un módulo de gestión de conomicimiento (Knowledge managment) y uno de visualización. Se cuenta, además, con dos bases de datos, una orientada a documentos\cite{han2011survey}(Document knowledge consolidation) y una ElasticSearch\cite{gormley2015elasticsearch} para visualización con Kibana \cite{gupta2015kibana}.

\begin{figure}
  \centering
  \includegraphics[width=16cm, keepaspectratio]{img/train_lexicon}
  \caption{Arquitectura de generación de lexicones}
  \label{fig:arquitectura}
\end{figure}

\subsection{Módulo ETL} 
\label{sec:etl}

El módulo ETL se encarga de la obtención y el preprocesado del corpus de artículos médicos. Se divide a su vez en dos submódulos: Corpus processor y Reputation calculator.
Corpus processor obtiene artículos de Pubmed Central\footnote{\url{https://www.ncbi.nlm.nih.gov/pmc/}} utilizando técnicas de web scrapping \cite{mitchell2018web}. Estos artículos en formato XML \cite{bray2000extensible}, son parseados y almacenados en formato JSON \cite{crockford2006application} en Document knowledge consolidation con una estructura de párrafos y frases, eliminando tablas, gráficas u otros artefactos no aprovechables por el sistema.
Una vez un documento se almacena en la base de datos, se procede a calcular su resumen automático aplicando el clásico algoritmo Text Rank \cite{mihalcea2004textrank} utilizando una popular implementación en Python \cite{DBLP:journals/corr/BarriosLAW16}. Este tipo de resúmenes ofrecen una ordenación de las frases de un documento por relevancia, sirviendo como factor de filtrado de información poco relevante o redundante y a la vez como factor de compresión, haciendo la información más manejable en memoria. Se obtiene un 20 porciento del tamaño original de los artículos y se almacena como un nuevo artibuto del documento en la base de datos.

Una vez preprocesados y almacenados los artículos, Reputation calculator calcula la reputación de los mismos para que esta sea añadida como nuevo atributo y ser utilizada posteriormente por el módulo Knowledge managment.
El algoritmo de reputación empleado es una adaptación del introducido en el artículo \cite{fernandez2018unified}. La reputación a priori de un artículo viene dada por:
\begin{equation}
\label{eq:1}
	rep_{p} = \alpha \cdot rep\_authors_{p} + (1 - \alpha) \cdot citations_{p} \,,
\end{equation}

donde  $rep\_authors_{p}$ es la reputación media del los autores del artículo y $citations_{p}$ su número de citas total en el momento de la consulta. El parámetro $\alpha \in (0,1)$ actúa como modulador de importancia relativa entre las citas y los autores. La reputación de cada autor viene dada por:
\begin{equation}
\label{eq:3}
	rep_{i} = \omega_1 \cdot inf\_citation\_count + \omega_2 \cdot citation\_velocity + \omega_3 \cdot seniority + \omega_4 \cdot papers \,,
\end{equation}

donde $\sum_{i=1}^4\omega_i=1$. El parámetro $inf\_citation\_count$ representa el número de citas altamente influyentes del autor. \cite{valenzuela2015identifying}. $citation\_velocity$ indica lo popular que es el autor durante los últimos 3 años. $seniority$ es la cantidad de años transcurridos entre la primera y última publicación del autor. Por último, $papers$ es el número total de artículos publicados por el autor. 
Estos parámetros son extraídos de  la API REST de Semantic Scholar\cite{semanticscholar2018sc}. Una vez que el módulo Reputation Calculator ha calculado la reputación del artículo, esta se añade a \emph{Document knowledge consolidation}, como un atributo nuevo.

\subsection{Módulo de Gestión del conocimiento} 
\label{sec:know}
Este módulo consulta \emph{Document knowledge consolidation} y sirviéndose de 3 submódulos, construye finalmente el lexicón. Estos 3 submódulos son Text filter, Text normalizer y Lexicon builder.

\subsection{Text filter} 
\label{sec:filter}

Este módulo analiza los resúmenes extraidos por el módulo ETL por cada documento, aplicando técnicas de procesamiento de lenguaje natural (NLP) \cite{jurafsky2014speech} extrayendo los sustantivos y eliminando palabras vacías \cite{chandramouli2018domain}, tanto genéricas como de dominio médico\cite{gupta2015distantly} y académico\cite{seinecleStopwords2016}. Finalmente, se obtienen los lemas de los sustantivos, lo que permite cierta desambiguación, ya que el lema de una palabra depende de la función sintáctica de la misma. Además, se resuelven posibles conflictos entre mayúsculas, minúsculas, singulares y plurales.

\subsection{Text normalizer} 
\label{sec:filter}
Este submódulo construye una matriz de términos por documentos \cite{anandarajan2019term} a partir de la cuál construye la matriz TF-IDF \cite{ramos2003using}. Los pesos resultantes se combinan con las reputaciones de los artículos dando la medida de relevancia de cada término $rel\_lex_{t}$. La relevancia de cada término $t$ en los  $N$ artículos pertenecientes al corpus $C$ viene dada por:

\begin{equation}
	rel\_lex_{t} =\log\left(\frac{1}{N} \cdot \sum \limits_{p=1}^N \beta  \cdot tfidf(t)_p + (1-\beta) \cdot rep_p\right), \forall p \in C \,,
\end{equation}

donde $rep_p$ es la reputación del artículo $p$, $tfidf(t)_p$ es el valor TF-IDF del término $t$ en el artículo $p$ y $\beta \in (0,1)$ es otro parámetro que modula la importancia relativa del valor TF-IDF sobre la reputación. Cabe destacar que $tfidf(t)_p \in (0,1)$ ya que han sido normalizados por simplicidad y se aplica el logaritmo para normalizar la distribución.

\subsection{Lexicon builder} 
\label{sec:filter}
Este componente construye y organiza el lexicón a partir del texto normalizado. Contempla además la posibilidad de ponderar aún más el peso de aquellos términos de dominio específico proporcionados por un diccionario, médico en este caso \cite{webster2017merriam}. 

Se organizan y construyen lexicones por cada conjunto de artículos correspondientes a cada año disponible, teniendo en cuenta la evolución de la relevancia de cada término a lo largo de los años. De esta manera se puede modular la curva de olvido \cite{averell2011form} y tener en consideración las tendencias del dominio de aplicación.


Se construyen diferentes valores de relevancia para cada término específico $rel\_lex_{t}(y)$ de acuerdo a un año específico $y$, manteniendo las palabras del año anterior en el nuevo de la forma:

\begin{equation}
	rel\_lex_{t}(y) = \rho \cdot rel\_lex_{t} + (1-\rho)\cdot rel\_lex_{t}(y-1) \,,
\end{equation}

donde $rel\_lex_{t}$ es la relevancia proporcionada por el marco de trabajo para el término $t$ y $rel\_lex_{t}(y-1)$ es la correspondiente al año anteruir $y-1$. El parámetro $\rho$ controla el peso del año anterior.

\section{Creación del la red neuronal} 
\label{sec:lex}

Por muy grande que sea el corpus que se utilice para la creación de los lexicones, es virtualmente imposible contar con la relevancia de todos los términos existentes. Por tanto, se ha desarrollado una Red Neuronal Convolucional (CNN, por sus siglas en inglés)\cite{poria2015deep} para servir de apoyo al sistema. El propósito de la red es predecir la relevancia de aquellas frases que no contengan ninguna palabra presente en el lexicón. En la Tabla \ref{tab:cnn} se puede ver la configuración de la misma, la cuál es una versión optimizada de la aproximación introducida en \cite{bhavsar2017natural}. Cabe destacar que la capa de \textit{embedding} utiliza un modelo pre-entrenado de Glove \cite{pennington2014glove} con un vocabulario de  $400,000$ palabras de  Wikipedia \cite{o2016wikipedia}. La capa de salida cuenta con activación \textit{softmax}, la cuál proporciona valores entre $0$ y $1$. En las capas ocultas se cuenta con convoluciones, funciones de activación  \textit{relu} y funciones de \textit{dropout}.

Para construir la red, se propone la siguiente metodología: En primer lugar, el usuario selecciona un conjunto de artículos del corpus que no hayan sido utilizados para construir el lexicón. Después, se  procesa el texto separando por frases y términos, eliminando palabras vacías y lematizando de manera análoga a como se procede en el módulo de ETL, de esta manera se aumenta el número potencial de coincidencias entre el texto utilizado para la creación de la red y las palabras del lexicón. Se etiquetarán como relevantes ($1$) aquellas frases con mayor relevancia de acuerdo al lexicón y como no relevantes ($0$) las menor relevancia o aquellas que no contengan palabras del lexicón. Se define, por tanto, 
un umbral de relevancia $\epsilon$ para elegir el mínimo necesario de acuerdo a lo estricta que se quiere que sea la red neuronal. Finalmente se debe elegir el número de frases etiquetadas para conformar el conjunto de entrenamiento y test. La red neuronal resultante debería ser capaz de prececir la relevancias de las frases sin palabras del lexicón.
\begin{table}[H]
	\begin{center}
		\begin{tabular}{c}
			\hline
			\textbf{Layers} \\
            \hline
			\multicolumn{1}{l}{1. Embedding input\_dim 400000 output\_dim 50} \\
			\multicolumn{1}{l}{2. Dropout rate 0.4} \\
			\multicolumn{1}{l}{3. Conv1D 250 filters of 3 with stride 1} \\
            \multicolumn{1}{l}{4. Pool1D (max) with stride 1} \\
            \multicolumn{1}{l}{5. Dense units 250} \\
            \multicolumn{1}{l}{6. Dropout rate 0.4} \\
            \multicolumn{1}{l}{7. Relu} \\
            \multicolumn{1}{l}{8. Dense units 1} \\
            \multicolumn{1}{l}{9. Softmax} \\
			\hline
		\end{tabular}
	\end{center}
	\caption{Layers of the Convolutional Neural Network.}
	\label{tab:cnn}
\end{table}

\section{Estimación de relevancias de artículos} 

Se ha diseñao un flujo de trabajo para ilustrar como funciona todo el proceso del marco de trabajo. Se comienza eligiendo un texto para evaluar y se concluye devolviendo su relevancia normalizada entre $0$ y $1$ (ver Figura. \ref{fig:pipeline}).

\label{sec:full_process}
\begin{figure}
  \centering
  \includegraphics[width=8cm, keepaspectratio]{img/pipeline_2}
  \caption{Flujo de trabajo de cálculo de relevancia de un artículo}
  \label{fig:arquitectura}
\end{figure}





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% RESULTADOS %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\cleardoublepage
\chapter{Experimentos y resultados}
\label{chap:resus}



\section{Planificación temporal}
\label{sec:planificacion-temporal}
En la Figura \ref{fig:gantt} se puede ver un diagrama de Gantt\cite{wilson2003gantt} que refleja el tiempo empleado en cada una de las fases del proyecto.

\begin{figure}
  \centering
  \includegraphics[width=16cm, keepaspectratio]{img/gantt.png}
  \caption{Diagrama de Gantt del desarrollo}
  \label{fig:gantt}
\end{figure}   

\section{Experimentos}
\label{sec:experimentos}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% CONCLUSIONES %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\cleardoublepage
\chapter{Conclusiones}
\label{chap:conclusiones}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APÉNDICE(S) %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% BIBLIOGRAFIA %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\cleardoublepage

% Las siguientes dos instrucciones es todo lo que necesitas
% para incluir las citas en la memoria
\bibliographystyle{unsrt}
\bibliography{memoria}  % memoria.bib es el nombre del fichero que contiene
% las referencias bibliográficas. Abre ese fichero y mira el formato que tiene,
% que se conoce como BibTeX. Hay muchos sitios que exportan referencias en
% formato BibTeX. Prueba a buscar en http://scholar.google.com por referencias
% y verás que lo puedes hacer de manera sencilla.
% Más información: 
% http://texblog.org/2014/04/22/using-google-scholar-to-download-bibtex-citations/

\end{document}
